This repository provides a flexible, end-to-end pipeline for systematically exploring how different hyperparameter choices impact model performance. It includes:

Configurable hyperparameter sets (learning rate, batch size, optimizer type, dropout rate, etc.)

Random sampling or grid-search over those configurations

Automated training of multiple model instances

Visualization of training vs. validation loss and accuracy curves

By default, the example scripts demonstrate this workflow on the Fashion-MNIST dataset with a simple neural network, but you can easily adapt the code to any dataset or architecture. Use it as a template for:

Rapid prototyping of new hyperparameter ranges

Benchmarking different optimizers or regularization strategies

Understanding trade-offs between convergence speed and generalization

Clone the repo, install dependencies from requirements.txt, and modify the hyperparameter lists or model definition to fit your own tasks.
